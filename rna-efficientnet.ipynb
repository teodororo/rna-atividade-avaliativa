{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nImportação do repositório\n'''\n!git clone https://github.com/teodororo/rna-atividade-avaliativa.git","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:01:54.291166Z","iopub.execute_input":"2023-03-13T08:01:54.292065Z","iopub.status.idle":"2023-03-13T08:02:00.605004Z","shell.execute_reply.started":"2023-03-13T08:01:54.292036Z","shell.execute_reply":"2023-03-13T08:02:00.603209Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'rna-atividade-avaliativa'...\nremote: Enumerating objects: 2466, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 2466 (delta 2), reused 5 (delta 1), pack-reused 2459\u001b[K\nReceiving objects: 100% (2466/2466), 59.67 MiB | 18.53 MiB/s, done.\nResolving deltas: 100% (75/75), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nImportação de bibliotecas\n'''\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB7,preprocess_input as b7_preprocess_input\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.optimizers import SGD\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom itertools import islice\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:02:00.607888Z","iopub.execute_input":"2023-03-13T08:02:00.608474Z","iopub.status.idle":"2023-03-13T08:02:07.499650Z","shell.execute_reply.started":"2023-03-13T08:02:00.608383Z","shell.execute_reply":"2023-03-13T08:02:07.498570Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"'''\nVariaveis constantes\n'''\nTEST_PATH = '/kaggle/working/rna-atividade-avaliativa/dataset_partition/test/'\nTRAIN_PATH = '/kaggle/working/rna-atividade-avaliativa/dataset_partition/train/'\nVAL_PATH = '/kaggle/working/rna-atividade-avaliativa/dataset_partition/validation/'\nnum_classes = 15\nimg_size = (224,224)\n\n'''\nHiperparâmetros \n'''\nbatch_size = 6 # preferencialmente um multiplo de 2\nweights = 'imagenet' # pode ser None (pesos aleatórios)\ntrainable_layers = False # se for True, ele ajusta tudo\nepocas = 20\n\n'''\nHiperparâmetros da camada extra do output\n'''\n_optimizer = keras.optimizers.Adam()\n_activation = 'softmax'","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:02:07.501061Z","iopub.execute_input":"2023-03-13T08:02:07.502375Z","iopub.status.idle":"2023-03-13T08:02:09.712249Z","shell.execute_reply.started":"2023-03-13T08:02:07.502329Z","shell.execute_reply":"2023-03-13T08:02:09.711190Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"'''\nAjuste da base de dados\n'''\n\ntrain_datagen = ImageDataGenerator(preprocessing_function=b7_preprocess_input,rescale=1./255)\nval_datagen = ImageDataGenerator(preprocessing_function=b7_preprocess_input,rescale=1./255)\ntest_datagen = ImageDataGenerator(preprocessing_function=b7_preprocess_input,rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        TRAIN_PATH,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='categorical')\n\nval_generator = val_datagen.flow_from_directory(\n        VAL_PATH,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n        TEST_PATH,\n        target_size=img_size,\n        batch_size=batch_size,\n        class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:02:09.715120Z","iopub.execute_input":"2023-03-13T08:02:09.715878Z","iopub.status.idle":"2023-03-13T08:02:10.031186Z","shell.execute_reply.started":"2023-03-13T08:02:09.715833Z","shell.execute_reply":"2023-03-13T08:02:10.030171Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 1648 images belonging to 15 classes.\nFound 239 images belonging to 15 classes.\nFound 448 images belonging to 15 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model():\n    base_model = EfficientNetB7(\n        input_shape=img_size + (3,),\n        include_top=False,\n        weights=weights\n    )\n    base_model.trainable = False\n\n    model = keras.Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(num_classes, activation=_activation)\n    ])\n\n    return model\n\nmodel = create_model()\nmodel.compile(\n    optimizer=_optimizer,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:02:10.034263Z","iopub.execute_input":"2023-03-13T08:02:10.034737Z","iopub.status.idle":"2023-03-13T08:02:32.018574Z","shell.execute_reply.started":"2023-03-13T08:02:10.034698Z","shell.execute_reply":"2023-03-13T08:02:32.017556Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n258076736/258076736 [==============================] - 11s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"#checkpoint = ModelCheckpoint('best_weights.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=epocas,\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    #callbacks=[checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-13T08:02:32.019976Z","iopub.execute_input":"2023-03-13T08:02:32.020351Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"2023-03-13 08:02:51.191287: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/efficientnetb7/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"275/275 [==============================] - 50s 92ms/step - loss: 2.8547 - accuracy: 0.0716 - val_loss: 2.8134 - val_accuracy: 0.0753\nEpoch 2/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.8363 - accuracy: 0.0704 - val_loss: 2.8322 - val_accuracy: 0.0544\nEpoch 3/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.8402 - accuracy: 0.0692 - val_loss: 2.8954 - val_accuracy: 0.0753\nEpoch 4/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.8051 - accuracy: 0.0783 - val_loss: 2.8786 - val_accuracy: 0.0628\nEpoch 5/200\n275/275 [==============================] - 22s 80ms/step - loss: 2.8133 - accuracy: 0.0783 - val_loss: 2.7603 - val_accuracy: 0.0962\nEpoch 6/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.8180 - accuracy: 0.0777 - val_loss: 2.7637 - val_accuracy: 0.0837\nEpoch 7/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.8047 - accuracy: 0.0819 - val_loss: 2.8326 - val_accuracy: 0.0753\nEpoch 8/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.8002 - accuracy: 0.0789 - val_loss: 2.7038 - val_accuracy: 0.0921\nEpoch 9/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.8159 - accuracy: 0.0783 - val_loss: 2.7359 - val_accuracy: 0.0711\nEpoch 10/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.8013 - accuracy: 0.0795 - val_loss: 2.9380 - val_accuracy: 0.0460\nEpoch 11/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.7904 - accuracy: 0.0874 - val_loss: 2.8138 - val_accuracy: 0.0753\nEpoch 12/200\n275/275 [==============================] - 20s 73ms/step - loss: 2.8013 - accuracy: 0.0922 - val_loss: 2.8117 - val_accuracy: 0.1046\nEpoch 13/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.7894 - accuracy: 0.0843 - val_loss: 2.6891 - val_accuracy: 0.0795\nEpoch 14/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.8132 - accuracy: 0.0934 - val_loss: 2.7471 - val_accuracy: 0.1004\nEpoch 15/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.7877 - accuracy: 0.0934 - val_loss: 2.8290 - val_accuracy: 0.0711\nEpoch 16/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7975 - accuracy: 0.0934 - val_loss: 2.6900 - val_accuracy: 0.0879\nEpoch 17/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7870 - accuracy: 0.0856 - val_loss: 2.7588 - val_accuracy: 0.0753\nEpoch 18/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7921 - accuracy: 0.0904 - val_loss: 2.9467 - val_accuracy: 0.0795\nEpoch 19/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7776 - accuracy: 0.0892 - val_loss: 2.7874 - val_accuracy: 0.1004\nEpoch 20/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7974 - accuracy: 0.0941 - val_loss: 2.7896 - val_accuracy: 0.0837\nEpoch 21/200\n275/275 [==============================] - 20s 72ms/step - loss: 2.7856 - accuracy: 0.0928 - val_loss: 2.7826 - val_accuracy: 0.0544\nEpoch 22/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7826 - accuracy: 0.0934 - val_loss: 2.7568 - val_accuracy: 0.0544\nEpoch 23/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7775 - accuracy: 0.0886 - val_loss: 2.7120 - val_accuracy: 0.0879\nEpoch 24/200\n275/275 [==============================] - 19s 69ms/step - loss: 2.7860 - accuracy: 0.0904 - val_loss: 2.7046 - val_accuracy: 0.1046\nEpoch 25/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7609 - accuracy: 0.0892 - val_loss: 2.9286 - val_accuracy: 0.0795\nEpoch 26/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7989 - accuracy: 0.0941 - val_loss: 2.7460 - val_accuracy: 0.0962\nEpoch 27/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7783 - accuracy: 0.0825 - val_loss: 2.8564 - val_accuracy: 0.0544\nEpoch 28/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7950 - accuracy: 0.0904 - val_loss: 2.7840 - val_accuracy: 0.0544\nEpoch 29/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7744 - accuracy: 0.0874 - val_loss: 2.7507 - val_accuracy: 0.0879\nEpoch 30/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7860 - accuracy: 0.0971 - val_loss: 2.7413 - val_accuracy: 0.1213\nEpoch 31/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7668 - accuracy: 0.1001 - val_loss: 2.9086 - val_accuracy: 0.0837\nEpoch 32/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7854 - accuracy: 0.0910 - val_loss: 2.7579 - val_accuracy: 0.1088\nEpoch 33/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7767 - accuracy: 0.0977 - val_loss: 2.7147 - val_accuracy: 0.1088\nEpoch 34/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7866 - accuracy: 0.0886 - val_loss: 2.8023 - val_accuracy: 0.0962\nEpoch 35/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7750 - accuracy: 0.0886 - val_loss: 2.7312 - val_accuracy: 0.0795\nEpoch 36/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7740 - accuracy: 0.0898 - val_loss: 2.7270 - val_accuracy: 0.0586\nEpoch 37/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7640 - accuracy: 0.0868 - val_loss: 2.6809 - val_accuracy: 0.0921\nEpoch 38/200\n275/275 [==============================] - 20s 71ms/step - loss: 2.7645 - accuracy: 0.0886 - val_loss: 2.7887 - val_accuracy: 0.0795\nEpoch 39/200\n275/275 [==============================] - 19s 71ms/step - loss: 2.7819 - accuracy: 0.0916 - val_loss: 2.7471 - val_accuracy: 0.1004\nEpoch 40/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7637 - accuracy: 0.0928 - val_loss: 2.7577 - val_accuracy: 0.0586\nEpoch 41/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7629 - accuracy: 0.0971 - val_loss: 2.6873 - val_accuracy: 0.0837\nEpoch 42/200\n275/275 [==============================] - 19s 70ms/step - loss: 2.7825 - accuracy: 0.0922 - val_loss: 2.8176 - val_accuracy: 0.0502\nEpoch 43/200\n275/275 [==============================] - 22s 81ms/step - loss: 2.7826 - accuracy: 0.0874 - val_loss: 2.7141 - val_accuracy: 0.1046\nEpoch 44/200\n218/275 [======================>.......] - ETA: 3s - loss: 2.7917 - accuracy: 0.1087","output_type":"stream"}]},{"cell_type":"code","source":"test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)\npredictions = model.predict(test_generator, steps=test_steps_per_epoch)\npredicted_classes = np.argmax(predictions, axis=1)\ntrue_classes = test_generator.classes\nclass_labels = list(test_generator.class_indices.keys())\nreport = classification_report(true_classes, predicted_classes, target_names=class_labels,zero_division=0)\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_classes = test_generator.classes\ntest_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)\npredictions = model.predict(test_generator, steps=test_steps_per_epoch)\npredicted_classes = np.argmax(predictions, axis=1)\nconf_matrix = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize=(10,8))\nsns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Classe prevista')\nplt.ylabel('Classe verdadeira')\nplt.title('Matriz de confusão')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}